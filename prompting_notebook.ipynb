

Import required installs

pip install selenium
pip install webdriver-manager

Scrape Websites

import json
import openai
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, NoSuchElementException

def is_valid_article_link(url):
    # Filter out non-article URLs
    if "beap.gemini.yahoo.com" in url:
        return False
    return True

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
wait = WebDriverWait(driver, 10)
main_page = "https://finance.yahoo.com/topic/stock-market-news"
driver.get(main_page)

# Scroll to the end of the page to ensure all dynamic content is loaded
# body = driver.find_element(By.TAG_NAME, 'body')
# for _ in range(3):  # Adjust the number for more/less scrolling as needed
#     body.send_keys(Keys.PAGE_DOWN)
#     time.sleep(1)  # Wait for the page to load

articles_data = []

try:
    news_items = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'li[class*="js-stream-content"]')))
    for item in reversed(news_items):  # Iterate from bottom to top for stability
        data = {'title': '', 'link': '', 'content': 'Content not available.'} 
        try:
            title_element = item.find_element(By.TAG_NAME, 'h3')
            link = title_element.find_element(By.XPATH, './/ancestor::a').get_attribute('href')
            
            if not is_valid_article_link(link):
                print("Skipping non-article link")
                continue

            data['title'] = title_element.text
            data['link'] = link

            # Open a new tab to avoid losing the list of news_items
            driver.execute_script("window.open('');")
            driver.switch_to.window(driver.window_handles[1])
            driver.get(link)
            try:
                caas_body = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.caas-body')))
                paragraphs = caas_body.find_elements(By.TAG_NAME, 'p')
                article_text = ' '.join([paragraph.text for paragraph in paragraphs if paragraph.text])
                data['content'] = article_text if article_text else "Content not available."
            except (TimeoutException, NoSuchElementException):
                print(f"Content not found or page timed out for: {link}")
            
            articles_data.append(data)
            
            # Close the current tab and switch back to the main page tab
            driver.close()
            driver.switch_to.window(driver.window_handles[0])
        except Exception as e:
            print(f"Error processing article: {e}")
            # Ensure to close any opened tab in case of error and switch back
            if len(driver.window_handles) > 1:
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
finally:
    driver.quit()
    with open('articles_data.json', 'w', encoding='utf-8') as f:
        json.dump(articles_data, f, ensure_ascii=False, indent=4)
    print("Data has been saved to 'articles_data.json'")

Setup the prompts

persona = "You will response as if you are an optimistic, expert stock market analyst on Twitter. You have a deep understanding of financial markets. Respond in the manner of a catchy, hyped up, and detailed Tweet, with relevant hashtags, if needed. Give a definitive recommendation in the response. Limit your response to 280 characters maximum, and only two hastags maximum."
scraping = "Only us the following articles given in the following context to answer any questions:"
context = "Give me a Tweet"
AIMessages = []
AIMessages.append({"role": "system", "content": persona})
AIMessages.append({"role": "system", "content": scraping})
for elem in articles_data:
    AIMessages.append({"role": "system", "content": elem['content']})
AIMessages.append({"role": "user", "content": context})

Prompt the AI Influencer

response = openai.chat.completions.create(
        model = 'gpt-4-1106-preview',
        temperature = 1,
        messages=AIMessages,
     )
AITweetString = response.choices[0].message.content
print(AITweetString)

Method for Connecting to Twitter API

from requests_oauthlib import OAuth1Session

def post_tweet(tweet_text):
    # Your consumer keys and access tokens
    consumer_key = 'd0k61MNQLHYHMsEpCxPLLVFnK'
    consumer_secret = 'JUzZOoNsOaq1E55lidvrGzZdbVqTWzAbSTHgRlaByBsGXbhCPq'
    access_token = '1759005396788731905-6YjvkWBUZlPmkqoK1YEuBcNnr9cOfE'
    access_token_secret = 'lbsnnthJS50pOO2bWfUIlS8OjooVIJtfzOvPSi7zFnPTt'

    # Creating an OAuth1Session object
    oauth = OAuth1Session(
        consumer_key,
        client_secret=consumer_secret,
        resource_owner_key=access_token,
        resource_owner_secret=access_token_secret
    )

    # The URL for creating a Tweet
    tweet_url = 'https://api.twitter.com/2/tweets'

    # The payload of the tweet, using the parameter tweet_text
    tweet_payload = {"text": tweet_text}

    # Making the POST request to create a tweet
    response = oauth.post(tweet_url, json=tweet_payload)

    # Checking if the request was successful
    if response.status_code == 201:
        print("Tweet created successfully!")
        # Print the response from Twitter
        print(response.json())
    else:
        print(f"Failed to create tweet. Status code: {response.status_code}, Response text: {response.text}")

Post to Twitter

post_tweet(AITweetString)

